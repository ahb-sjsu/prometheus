# AI Safety Background

## Research Program (2024-Present)

I've developed an independent AI safety research program focused on falsifiable alignment verification, producing frameworks, tools, and empirical results:

**Philosophy Engineering & The AI Safety Stack**
An 8-layer architecture for ethics verification, from foundational pragmatism through machine-checkable specifications to deployment decisions. Introduces the Bond Invariance Principle—a falsifiability mechanism testing whether AI judgment maintains coherence under semantic perturbation.
- Framework: https://github.com/ahb-sjsu/erisml-lib/blob/main/AI_SAFETY_STACK.md

**SQND: Stratified Quantum Normative Dynamics**
Empirical measurement of moral reasoning structure in LLMs using symmetry-based coherence detection. Extensive experimental validation including Bell tests, contextuality experiments, holonomy measurements, and recursive self-probing across multiple model families.
- Theory & Experiments: https://github.com/ahb-sjsu/prometheus/tree/main/SQND-docs
- Measurement Tool: https://github.com/ahb-sjsu/sqnd-probe

**DEME: Democratic Ethics Module Engine**
Three-layer safety architecture (Reflex/Tactical/Strategic) for scalable ethical oversight, with 9-dimensional MoralVector assessment and cryptographic audit trails.
- Implementation: https://github.com/ahb-sjsu/erisml-lib

**ErisML: Governed Agent Specification Language**
Formal grammar for machine-checkable norm constraints on foundation-model-enabled agents. Includes longitudinal safety metrics (Norm Violation Rate, Alignment Drift Velocity).
- Repository: https://github.com/ahb-sjsu/erisml-lib

## Publications

**Under Review:**
- "DEME 2.0: Democratic Ethics Module Engine" — Submitted to Nature Machine Intelligence, Dec 2025
- "ErisML: A Modeling Language for Governed, Foundation-Model-Enabled Agents" — Submitted to IEEE Transactions on AI, Dec 2025

**Preprints & Technical Reports (2025-2026):**
- Stratified Quantum Normative Dynamics (v1-v6)
- Non-Abelian SQND Bond 2026
- Noether Ethics
- Tensorial Ethics (multi-chapter series)
- No Escape: Mathematical Containment for AI
- Epistemic Invariance Principle & I-EIP Monitor Whitepaper

All available at: https://github.com/ahb-sjsu/erisml-lib

## Research Infrastructure

**AGI-HPC**: Safety-aware cognitive architecture deployed on SJSU's HPC cluster (A100/H100/P100 GPUs) with three-layer safety verification.
- https://github.com/ahb-sjsu/agi-hpc

**Bond Index Calibration Suite**: 10,500+ adversarial test cases across 18 parametric transforms for alignment verification.

## Academic Position

**Lecturer, Computer Science** — Sonoma State University (2024-Present)
**Faculty, Computer Science & Engineering** — San Jose State University (2016-Present)

Graduate thesis advisor supervising student research in AI systems and safety. Courses in software engineering, cloud systems, and data analytics provide context for understanding deployed AI infrastructure.

## Technical Foundation

- Primary language: Python
- LLM integration: MCP server implementation for Claude, multi-model evaluation pipelines
- Formal methods: Language grammars (Lark), type systems (Pydantic), verification frameworks
- Multi-agent systems: PettingZoo RL adapter, norm-gating runtime

## Self-Directed Study

Extensive engagement with AI safety literature including Anthropic's published research on constitutional AI, interpretability, and model evaluations. My work on symmetry-based coherence detection was partly inspired by the interpretability agenda's focus on making model internals legible—I'm approaching a related problem (normative consistency) from the behavioral/black-box direction.
